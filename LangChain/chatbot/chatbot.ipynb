{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cae801f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key: AIzaSyD6gGfwQXUtiHKCLDi_MYy80wze001XdgA\n",
      "Chroma Telemetry: FALSE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Option A: get a single var\n",
    "print(\"Google API Key:\", os.getenv(\"GOOGLE_API_KEY\"))\n",
    "print(\"Chroma Telemetry:\", os.getenv(\"CHROMA_TELEMETRY\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba8dc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Point at a small local file from your repo to test\n",
    "# sample_file = Path.cwd() / \"../aso-enrichment-tool\" / \"CLAUDE.md\"\n",
    "# docs = TextLoader(str(sample_file), autodetect_encoding=True).load()\n",
    "\n",
    "# # Use a code-aware splitter if you want, e.g. Python\n",
    "# splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "#     language=Language.MARKDOWN, chunk_size=1200, chunk_overlap=150\n",
    "# )\n",
    "# chunks = splitter.split_documents(docs)\n",
    "# print(chunks[0].page_content[:1000])  # Print first 1000 characters of the first chunk\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75a4ade7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 75 Python files as documents\n",
      "Created 1952 chunks in total\n",
      "import os\n",
      "import json\n",
      "import logging\n",
      "from logging.handlers import RotatingFileHandler\n",
      "import pandas as pd\n",
      "from langchain.schema import HumanMessage\n",
      "from langchain_openai import ChatOpenAI\n",
      "from dotenv import load_dotenv\n",
      "from datetime import datetime\n",
      "from colorama import Fore, Style\n",
      "import sys\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "import re\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from config import (\n",
      "    AGENT_PROMPT_MD_GET_CONFIDENCE_SCORE,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "# 1️⃣ Path to repo folder\n",
    "repo_folder = Path.cwd().parent / \"aso-enrichment-tool\"\n",
    "\n",
    "# 2️⃣ DirectoryLoader with loader_args for TextLoader\n",
    "loader = DirectoryLoader(\n",
    "    path=str(repo_folder),\n",
    "    glob=\"**/*.py\",  # recursively load all Python files\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"autodetect_encoding\": True}  # <-- pass here\n",
    ")\n",
    "\n",
    "# 3️⃣ Load documents\n",
    "docs = loader.load()\n",
    "print(f\"Loaded {len(docs)} Python files as documents\")\n",
    "\n",
    "# 4️⃣ Split code into chunks\n",
    "splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON,\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = splitter.split_documents(docs)\n",
    "print(f\"Created {len(chunks)} chunks in total\")\n",
    "print(chunks[0].page_content[:1000])  # preview first chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d54d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1952\n",
      "Vectorstore created and retriever ready. tags=['Chroma', 'GoogleGenerativeAIEmbeddings'] vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x11ad30590> search_kwargs={'k': 10}\n"
     ]
    }
   ],
   "source": [
    "emb = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    chunks,\n",
    "    embedding=emb,\n",
    "    collection_name=\"my_vector_store\"\n",
    ")\n",
    "\n",
    "print(vectorstore._collection.count())\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "print(\"Vectorstore created and retriever ready.\", retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f5ace8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# 4. Gemini chat LLM\n",
    "# ===============================\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4939ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt = (\n",
    "\t\"You are an expert in code analysis and can help users understand the code snippets.\\n\"\n",
    "\t\"You can also compare data across files to identify similarities and differences.\\n\"\n",
    "\t\"Use the following pieces of retrieved context to answer the question. Use below format for interaction.\\n\"\n",
    "\t\"Question: {question}\\n\"\n",
    "\t\"Context: {context}\\n\"\n",
    "\t\"Answer:\"\n",
    ")\n",
    "prompt_template = PromptTemplate.from_template(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3534d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rag_chain = ({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}) | prompt_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c44d6c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer:\\n\\nThere are five scripts in the `utils` folder based on the provided code snippets:\\n\\n1. `format_script.py` (inferred from `from utils.format_script import CSVFormatProcessor`)\\n2. `merge_formatted_csv.py` (inferred from `from utils.merge_formatted_csv import merge_formatted_file`)\\n3. `find_missing_rows.py` (inferred from `from utils.find_missing_rows import main as find_missing_rows_main`)\\n4. `update_category.py` (inferred from `from utils.update_category import main as update_category_main`)\\n5. `merged_output_input_values.py` (inferred from `from utils.merged_output_input_values import main as merge_output_input_values`)\\n6. `format_script_distribution.py` (inferred from `from utils.format_script_distribution import main as format_script_distribution`)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"How many scripts are there in utils folder?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35817f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Assume you already have:\n",
    "# - retriever (from Chroma vectorstore)\n",
    "# - llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Create the conversational chain\n",
    "chatbot_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "# Function Gradio calls for each message\n",
    "def chat_fn(user_input, history):\n",
    "    global chat_history\n",
    "\n",
    "    # Pass tuples (user, assistant) to LangChain\n",
    "    result = chatbot_chain.invoke({\n",
    "        \"question\": user_input,\n",
    "        \"chat_history\": chat_history\n",
    "    })\n",
    "\n",
    "    answer = result[\"answer\"]\n",
    "\n",
    "    # Append new interaction\n",
    "    chat_history.append((user_input, answer))\n",
    "\n",
    "\n",
    "    # Optionally include first source doc for reference\n",
    "    sources = \"\"\n",
    "    if result.get(\"source_documents\"):\n",
    "        sources = \"\\n\\nSource snippet:\\n\" + result[\"source_documents\"][0].page_content[:300]\n",
    "    \n",
    "    return answer + sources\n",
    "\n",
    "# Build Gradio interface\n",
    "iface = gr.ChatInterface(fn=chat_fn, type=\"messages\",\n",
    "    title=\"Gemini RAG Chatbot\",\n",
    "    description=\"Ask questions about your repo code/docs.\"\n",
    ").launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
